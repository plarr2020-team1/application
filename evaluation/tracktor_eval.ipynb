{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "application_path = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(application_path, 'tools'))\n",
    "\n",
    "from monodepth2.infer import load_model\n",
    "from tracktor_utils import tracker_obj\n",
    "from tracktor.utils import interpolate\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, ToPILImage\n",
    "\n",
    "from monodepth2.infer import infer_depth as monodepth_infer\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import pickle \n",
    "\n",
    "from tqdm import trange\n",
    "from human_depth_dataset.dataset import (\n",
    "    RGBDPeopleDataset, \n",
    "    KittiHumanDepthDataset\n",
    ")\n",
    "\n",
    "from human_depth_dataset.evaluate_depth import evaluateDepths, \\\n",
    "    calculateTrueErrors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalmanfilter(x,p,z,r):\n",
    "    # p - estimate unceratininty \n",
    "    # r - measurement unceratininty ( Ïƒ2 )  \n",
    "    # z - Measured System State\n",
    "\n",
    "    # Kalman gain calculation\n",
    "    K =  p/(p+r)\n",
    "    # estimate current state\n",
    "    x1 = x + K*(z-x)\n",
    "    # update current estimate uncertainity\n",
    "    p1 = (1-K)*p\n",
    "\n",
    "    return (x1,p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = tracker_obj(os.path.join(application_path, \"tracking_wo_bnw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = open('../human_depth_dataset/scenes_with_min_2_people.txt').read()\n",
    "sequences = np.unique([s.split('/')[0] for s in sequences.split('\\n') if len(s.split('/')[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Loading model from  models/mono+stereo_1024x320\n",
      "   Loading pretrained encoder\n",
      "   Loading pretrained decoder\n"
     ]
    }
   ],
   "source": [
    "inference = {'name': 'monodepth'}\n",
    "\n",
    "encoder, depth_decoder, (feed_width, feed_height) = load_model(\"mono+stereo_1024x320\")\n",
    "inference['encoder'] = encoder\n",
    "inference['depth_decoder'] = depth_decoder\n",
    "inference['input_size'] = (feed_width, feed_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "findTransformECC() missing required argument 'inputMask' (pos 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2d9108125392>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         }\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/enis/application/tracking_wo_bnw/src/tracktor/tracker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m    298\u001b[0m                         \u001b[0;31m# align\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_align\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                         \u001b[0;31m# apply motion model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/enis/application/tracking_wo_bnw/src/tracktor/tracker.py\u001b[0m in \u001b[0;36malign\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m    205\u001b[0m                         \u001b[0mwarp_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                         \u001b[0mcriteria\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTERM_CRITERIA_EPS\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTERM_CRITERIA_COUNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_iterations\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermination_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                         \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarp_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindTransformECC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1_gray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim2_gray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarp_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarp_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m                         \u001b[0mwarp_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarp_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: findTransformECC() missing required argument 'inputMask' (pos 6)"
     ]
    }
   ],
   "source": [
    "depth_tracks = {}\n",
    "depth_tracks_smoothed = {}\n",
    "depth_tracks_p = {}\n",
    "\n",
    "depth_frame_dict = {}\n",
    "\n",
    "depth_merger = 'median'\n",
    "for seq in tqdm(sequences, leave=True):\n",
    "    depth_frame_dict[seq] = {}\n",
    "    tracker.reset()\n",
    "    \n",
    "    frames = glob(f'../human_depth_dataset/data/kitti/val/raw/{seq}_sync/image_02/data/*.png')\n",
    "    frames = sorted(frames, key=lambda x: int(x.split('/')[-1].split('.')[0])) \n",
    "    \n",
    "    for frame in frames:\n",
    "        if 'combined' in frame:\n",
    "            continue\n",
    "        depth_frame_dict[seq][frame.split('/')[-1]] = []\n",
    "        img_pil = Image.open(frame)\n",
    "        \n",
    "        depth_map, depth_im = monodepth_infer(inference['encoder'],\n",
    "                                                  inference['depth_decoder'],\n",
    "                                                  inference['input_size'],\n",
    "                                                  img_pil)\n",
    "        depth_map = depth_map[0, 0] * 5.4\n",
    "        \n",
    "        transforms = Compose([\n",
    "            Resize((749, 1333)),\n",
    "            ToTensor(),\n",
    "        ])\n",
    "        frame_batch = {\n",
    "            'img': transforms(img_pil).unsqueeze(0)[:, :3, :, :]#.cuda()\n",
    "        }\n",
    "                \n",
    "        tracker.step(frame_batch)\n",
    "        results = tracker.get_results()\n",
    "        results = interpolate(results)\n",
    "        for t, r in results.items():\n",
    "            x1, y1, x2, y2 = map(int, r[max(r, key=int)])\n",
    "            m = np.zeros_like(depth_map)\n",
    "            y1 = int(y1 * m.shape[0] / 749)\n",
    "            y2 = int(y2 * m.shape[0] / 749)\n",
    "\n",
    "            x1 = int(x1 * m.shape[1] / 1333)\n",
    "            x2 = int(x2 * m.shape[1] / 1333)\n",
    "\n",
    "            m[y1:y2, x1:x2] = 1\n",
    "            person_depth = depth_map * m\n",
    "            try:\n",
    "                if depth_merger == 'mean':\n",
    "                    avg_depth = person_depth[np.where(person_depth != 0)].mean()\n",
    "                elif depth_merger == 'median': \n",
    "                    avg_depth = np.median(person_depth[np.where(person_depth != 0)])\n",
    "                else:\n",
    "                    raise Exception(\"Undefined depth_merger error!\")\n",
    "                x, y = int((x1 + x2) / 2), int((y1 + y2) / 2)\n",
    "\n",
    "                if t not in depth_tracks:\n",
    "                    depth_tracks[t] = [avg_depth]\n",
    "                else: \n",
    "                    depth_tracks[t].append(avg_depth)\n",
    "                    \n",
    "                avg_depth_s = avg_depth\n",
    "                p = 1\n",
    "                if len(depth_tracks[t]) > 1:\n",
    "                    avg_depth_s = depth_tracks_smoothed[t][-1]\n",
    "                    p = depth_tracks_p[t][-1]\n",
    "                \n",
    "                avg_depth_s, p = kalmanfilter(avg_depth_s, p, avg_depth, 1)\n",
    "                \n",
    "                if t not in depth_tracks_smoothed:\n",
    "                    depth_tracks_smoothed[t] = [avg_depth_s]\n",
    "                else: \n",
    "                    depth_tracks_smoothed[t].append(avg_depth_s)\n",
    "                    \n",
    "                if t not in depth_tracks_p:\n",
    "                    depth_tracks_p[t] = [p]\n",
    "                else: \n",
    "                    depth_tracks_p[t].append(p)\n",
    "                    \n",
    "                depth_frame_dict[seq][frame.split('/')[-1]].append({\n",
    "                    'box': [x1, y1, x2, y2],\n",
    "                    'depth': avg_depth_s\n",
    "                })\n",
    "                \n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_dataset = KittiHumanDepthDataset(\n",
    "    '../human_depth_dataset/scenes_with_min_2_people.txt', \n",
    "    '../human_depth_dataset/data/kitti/val/',\n",
    "    '../human_depth_dataset/data/kitti/yolact.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "\n",
    "for i in trange(len(kitti_dataset)):\n",
    "    item = kitti_dataset.__getitem__(i)\n",
    "    mask = item['mask']['mask']\n",
    "    index = item['index']\n",
    "    depth = item['depth']\n",
    "    \n",
    "    seq, ind = index.split('/')   \n",
    "    tracktor_boxes = depth_frame_dict[seq[:-5]][ind]\n",
    "    \n",
    "    if len(mask):\n",
    "        matching = np.zeros((mask.shape[0], len(tracktor_boxes)))\n",
    "        \n",
    "        for ii, m in enumerate(mask):\n",
    "            for jj, box in enumerate(tracktor_boxes):\n",
    "                x1, y1, x2, y2 = box['box']\n",
    "                matching[ii][jj] = m[y1:y2, x1:x2].sum() / m.sum()#((y2 - y1) * (x2 - x1))\n",
    "        \n",
    "        mask_assign = {}\n",
    "        for jj, _ in enumerate(tracktor_boxes):\n",
    "            current_masks = list(set(range(len(mask))) - set(mask_assign.keys()))\n",
    "            if len(current_masks):\n",
    "                mask_thres = np.where(matching[current_masks, jj] > 0.3)\n",
    "                if len(mask_thres[0]):\n",
    "                    mask_assign[matching[mask_thres[0], jj].argmax()] = jj\n",
    "        \n",
    "        for m, t in mask_assign.items():\n",
    "            person_depth = depth * mask[m, :, :, 0]\n",
    "            gt = np.median(person_depth[np.where(person_depth != 0)])\n",
    "            predicted = tracktor_boxes[t]['depth']\n",
    "            stats.append(evaluateDepths(predicted, gt))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# # Create figure and axes\n",
    "# fig,ax = plt.subplots()\n",
    "\n",
    "# # Display the image\n",
    "# ax.imshow(mask[0, :, :, 0])\n",
    "\n",
    "# # Create a Rectangle patch\n",
    "# for b in tracktor_boxes:\n",
    "#     x1, y1, x2, y2 = b['box']\n",
    "#     rect = patches.Rectangle((x1,y1),(x2 - x1),(y2- y1),linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "#     # Add the patch to the Axes\n",
    "#     ax.add_patch(rect)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rect = patches.Rectangle((50,100),40,30,linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "# plt.imshow(mask[0, :, :, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 Soc Dist",
   "language": "python",
   "name": "socdist-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
