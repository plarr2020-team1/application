{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "application_path = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(application_path, 'tools'))\n",
    "\n",
    "from monodepth2.infer import load_model\n",
    "from tracktor_utils import tracker_obj\n",
    "from tracktor.utils import interpolate\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, ToPILImage\n",
    "\n",
    "from monodepth2.infer import infer_depth as monodepth_infer\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import pickle \n",
    "\n",
    "from tqdm import trange\n",
    "from human_depth_dataset.dataset import (\n",
    "    RGBDPeopleDataset, \n",
    "    KittiHumanDepthDataset\n",
    ")\n",
    "\n",
    "from human_depth_dataset.evaluate_depth import evaluateDepths, \\\n",
    "    calculateTrueErrors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalmanfilter(x,p,z,r):\n",
    "    # p - estimate unceratininty \n",
    "    # r - measurement unceratininty ( σ2 )  \n",
    "    # z - Measured System State\n",
    "\n",
    "    # Kalman gain calculation\n",
    "    K =  p/(p+r)\n",
    "    # estimate current state\n",
    "    x1 = x + K*(z-x)\n",
    "    # update current estimate uncertainity\n",
    "    p1 = (1-K)*p\n",
    "\n",
    "    return (x1,p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = tracker_obj(os.path.join(application_path, \"tracking_wo_bnw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Loading model from  models/mono+stereo_1024x320\n",
      "   Loading pretrained encoder\n",
      "   Loading pretrained decoder\n"
     ]
    }
   ],
   "source": [
    "inference = {'name': 'monodepth'}\n",
    "\n",
    "encoder, depth_decoder, (feed_width, feed_height) = load_model(\"mono+stereo_1024x320\")\n",
    "inference['encoder'] = encoder\n",
    "inference['depth_decoder'] = depth_decoder\n",
    "inference['input_size'] = (feed_width, feed_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [13:15<00:00, 265.31s/it]\n"
     ]
    }
   ],
   "source": [
    "depth_tracks = {}\n",
    "depth_tracks_smoothed = {}\n",
    "depth_tracks_p = {}\n",
    "\n",
    "depth_frame_dict = {}\n",
    "\n",
    "depth_merger = 'median'\n",
    "for seq in tqdm(sequences, leave=True):\n",
    "    depth_frame_dict[seq] = {}\n",
    "    tracker.reset()\n",
    "    \n",
    "    frames = glob(f'../human_depth_dataset/data/rgbd/rgb/*_{seq}.ppm')\n",
    "    frames = sorted(frames, key=lambda x: int(x.split('/')[-1].split('_')[1])) \n",
    "    \n",
    "    for frame in frames:\n",
    "        if 'combined' in frame:\n",
    "            continue\n",
    "        depth_frame_dict[seq][frame.split('/')[-1]] = []\n",
    "        img_pil = Image.open(frame)\n",
    "        \n",
    "        depth_map, depth_im = monodepth_infer(inference['encoder'],\n",
    "                                                  inference['depth_decoder'],\n",
    "                                                  inference['input_size'],\n",
    "                                                  img_pil)\n",
    "        depth_map = depth_map[0, 0] * 5.4\n",
    "        \n",
    "        transforms = Compose([\n",
    "            Resize((749, 1333)),\n",
    "            ToTensor(),\n",
    "        ])\n",
    "        frame_batch = {\n",
    "            'img': transforms(img_pil).unsqueeze(0)[:, :3, :, :]#.cuda()\n",
    "        }\n",
    "                \n",
    "        tracker.step(frame_batch)\n",
    "        results = tracker.get_results()\n",
    "        results = interpolate(results)\n",
    "        for t, r in results.items():\n",
    "            x1, y1, x2, y2 = map(int, r[max(r, key=int)])\n",
    "            m = np.zeros_like(depth_map)\n",
    "            y1 = int(y1 * m.shape[0] / 749)\n",
    "            y2 = int(y2 * m.shape[0] / 749)\n",
    "\n",
    "            x1 = int(x1 * m.shape[1] / 1333)\n",
    "            x2 = int(x2 * m.shape[1] / 1333)\n",
    "\n",
    "            m[y1:y2, x1:x2] = 1\n",
    "            person_depth = depth_map * m\n",
    "            try:\n",
    "                if depth_merger == 'mean':\n",
    "                    avg_depth = person_depth[np.where(person_depth != 0)].mean()\n",
    "                elif depth_merger == 'median': \n",
    "                    avg_depth = np.median(person_depth[np.where(person_depth != 0)])\n",
    "                else:\n",
    "                    raise Exception(\"Undefined depth_merger error!\")\n",
    "                x, y = int((x1 + x2) / 2), int((y1 + y2) / 2)\n",
    "\n",
    "                if t not in depth_tracks:\n",
    "                    depth_tracks[t] = [avg_depth]\n",
    "                else: \n",
    "                    depth_tracks[t].append(avg_depth)\n",
    "                    \n",
    "                avg_depth_s = avg_depth\n",
    "                p = 1\n",
    "                if len(depth_tracks[t]) > 1:\n",
    "                    avg_depth_s = depth_tracks_smoothed[t][-1]\n",
    "                    p = depth_tracks_p[t][-1]\n",
    "                \n",
    "                avg_depth_s, p = kalmanfilter(avg_depth_s, p, avg_depth, 1)\n",
    "                \n",
    "                if t not in depth_tracks_smoothed:\n",
    "                    depth_tracks_smoothed[t] = [avg_depth_s]\n",
    "                else: \n",
    "                    depth_tracks_smoothed[t].append(avg_depth_s)\n",
    "                    \n",
    "                if t not in depth_tracks_p:\n",
    "                    depth_tracks_p[t] = [p]\n",
    "                else: \n",
    "                    depth_tracks_p[t].append(p)\n",
    "                    \n",
    "                depth_frame_dict[seq][frame.split('/')[-1]].append({\n",
    "                    'box': [x1, y1, x2, y2],\n",
    "                    'depth': avg_depth_s\n",
    "                })\n",
    "                \n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RGBDPeopleDataset(Dataset):\n",
    "    def __init__(self, data_root='../data/rgbd/', mask_file='../data/rgbd/yolact.pkl'):\n",
    "        rgb_dir = 'rgb'\n",
    "        depth_dir = 'depth'\n",
    "\n",
    "        self.rgb_files = [os.path.join(data_root, rgb_dir, filename) for\n",
    "                          filename in\n",
    "                          os.listdir(os.path.join(data_root, rgb_dir)) if 'combined' not in filename]\n",
    "\n",
    "        self.depth_files = []\n",
    "        for path in self.rgb_files:\n",
    "            filename = os.path.splitext(os.path.basename(path))[0]\n",
    "            self.depth_files.append(\n",
    "                os.path.join(data_root, depth_dir, filename+'.pgm'))\n",
    "            \n",
    "        self.masks = None\n",
    "        if mask_file != None:\n",
    "            with open(mask_file, 'rb') as f:\n",
    "                self.masks = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_np = np.rot90(cv2.imread(self.rgb_files[idx]))\n",
    "        rgb_np = cv2.cvtColor(rgb_np, cv2.COLOR_BGR2RGB)\n",
    "        rgb_im = pil.fromarray(rgb_np)\n",
    "\n",
    "        depth = cv2.imread(self.depth_files[idx], -1).newbyteorder()\n",
    "\n",
    "        # According to the dataset paper: http://www2.informatik.uni-freiburg.de/~spinello/spinelloIROS11.pdf\n",
    "        depth = 8 * 0.075 * 594.2 / (1084 - depth)\n",
    "        depth = np.rot90(depth)\n",
    "        \n",
    "        index = self.rgb_files[idx].split('/')[-1]\n",
    "        mask = self.masks[index] if self.masks != None else None\n",
    "\n",
    "        return {'rgb': rgb_im, 'depth': depth, 'index': index, 'mask': mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbd_dataset = RGBDPeopleDataset( \n",
    "    '../human_depth_dataset/data/rgbd/',\n",
    "    '../human_depth_dataset/data/rgbd/yolact.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "import PIL.Image as pil\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seq0_0285_0.ppm'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3399/3399 [00:37<00:00, 91.20it/s]\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "\n",
    "for i in trange(len(rgbd_dataset)):\n",
    "    item = rgbd_dataset.__getitem__(i)\n",
    "    mask = item['mask']['mask']\n",
    "    index = item['index']\n",
    "    depth = item['depth']\n",
    "    \n",
    "    seq, ind, _ = index.split('_')   \n",
    "    tracktor_boxes = depth_frame_dict[int(index[-5])][index]\n",
    "    \n",
    "    if len(mask):\n",
    "        matching = np.zeros((mask.shape[0], len(tracktor_boxes)))\n",
    "        \n",
    "        for ii, m in enumerate(mask):\n",
    "            for jj, box in enumerate(tracktor_boxes):\n",
    "                x1, y1, x2, y2 = box['box']\n",
    "                matching[ii][jj] = m[y1:y2, x1:x2].sum() / m.sum()#((y2 - y1) * (x2 - x1))\n",
    "        \n",
    "        mask_assign = {}\n",
    "        for jj, _ in enumerate(tracktor_boxes):\n",
    "            current_masks = list(set(range(len(mask))) - set(mask_assign.keys()))\n",
    "            if len(current_masks):\n",
    "                mask_thres = np.where(matching[current_masks, jj] > 0.3)\n",
    "                if len(mask_thres[0]):\n",
    "                    mask_assign[matching[mask_thres[0], jj].argmax()] = jj\n",
    "        \n",
    "        for m, t in mask_assign.items():\n",
    "            person_depth = depth * mask[m, :, :, 0]\n",
    "            gt = np.median(person_depth[np.where(person_depth != 0)])\n",
    "            predicted = tracktor_boxes[t]['depth']\n",
    "            stats.append(evaluateDepths(predicted, gt))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pixels:  122.0\n",
      "    0.4696,     1.4129,     0.1829,     2.6743,     0.5131,     0.3115,     0.6230,     0.7869\n"
     ]
    }
   ],
   "source": [
    "calculateTrueErrors([s[0] for s in stats], [s[1] for s in stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# # Create figure and axes\n",
    "# fig,ax = plt.subplots()\n",
    "\n",
    "# # Display the image\n",
    "# ax.imshow(mask[0, :, :, 0])\n",
    "\n",
    "# # Create a Rectangle patch\n",
    "# for b in tracktor_boxes:\n",
    "#     x1, y1, x2, y2 = b['box']\n",
    "#     rect = patches.Rectangle((x1,y1),(x2 - x1),(y2- y1),linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "#     # Add the patch to the Axes\n",
    "#     ax.add_patch(rect)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rect = patches.Rectangle((50,100),40,30,linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "# plt.imshow(mask[0, :, :, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 Soc Dist",
   "language": "python",
   "name": "socdist-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
